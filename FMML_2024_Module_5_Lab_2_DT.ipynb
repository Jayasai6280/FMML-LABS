{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jayasai6280/FMML-LABS/blob/main/FMML_2024_Module_5_Lab_2_DT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x9mXnIyH_TU"
      },
      "source": [
        "# Lab 2\n",
        "# Classification II : Introduction to Decision Trees\n",
        "\n",
        "```\n",
        "Module Coordinator : Nikunj Nawal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of some property by inferring simple decision rules from the data features.\n",
        "\n",
        "\n",
        "Let us take a look at an example of a decision tree which predicts the class of the species of Iris flower from the iris dataset\n"
      ],
      "metadata": {
        "id": "lA8j3sciushf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRLg2DwJTqMN"
      },
      "source": [
        "#Importing the necessary packages\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8eLU5YOhiPH"
      },
      "source": [
        "### Code for the core experiment:\n",
        "\n",
        "- Creating the decision tree classifier based on parameters passed.\n",
        "- Evaluating the classifier's accuracy and plotting its confusion matrix.\n",
        "- Plotting its decision boundary.\n",
        "- Creating and showing the visualization of the tree made.\n",
        "\n",
        "**SKIP THE CODE IN THE FOLLOWING CELL FOR NOW AND COME BACK TO IT LATER AFTER UNDERSTANDING THE IDEA AND INTUITION BEHIND DECISION TREES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNo2x3Tbhhu3"
      },
      "source": [
        "def performExperiment(trainSet : tuple, testSet : tuple, max_depth : int = None, feature_names : list = None, class_names : list = None, criterion = \"gini\", min_samples_split : int = 2 , min_samples_leaf = 1):\n",
        "  #Importing the Decision tree classifier from sklearn:\n",
        "\n",
        "  clf = tree.DecisionTreeClassifier(max_depth = max_depth, \\\n",
        "                                    criterion = criterion,\\\n",
        "                                    min_samples_split = min_samples_split,\\\n",
        "                                    min_samples_leaf = min_samples_leaf,\\\n",
        "                                    splitter = \"best\",\\\n",
        "                                    random_state = 0,\\\n",
        "                                    )\n",
        "  X_train, y_train = trainSet\n",
        "  X_test, y_test = testSet\n",
        "\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy of the decision tree on the test set: \\n\\n{:.3f}\\n\\n\".format(accuracy_score(y_pred, y_test)))\n",
        "\n",
        "  print(\"Here is a diagram of the tree created to evaluate each sample:\")\n",
        "  fig, ax = plt.subplots(figsize=(12,10))\n",
        "  imgObj = tree.plot_tree(clf, filled=True, ax=ax, feature_names = feature_names, class_names = class_names, impurity=False, proportion=True, rounded=True, fontsize = 12)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def giveAnExample(n : int):\n",
        "  performExperiment((X_train, y_train),  (X_test, y_test), feature_names = iris[\"feature_names\"], class_names = iris[\"target_names\"], max_depth = n)\n",
        "\n",
        "def plotDecisionBoundary(X, y, pair, clf):\n",
        "  x_min, x_max = X[:, pair[0]].min() - 1, X[:, pair[0]].max() + 1\n",
        "  y_min, y_max = X[:, pair[1]].min() - 1, X[:, pair[1]].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                      np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "  y_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.contourf(xx, yy, y_pred, alpha=0.4)\n",
        "  plt.scatter(X[:, pair[0]], X[:, pair[1]], c = y, s = 50, edgecolor='k')\n",
        "  plt.title(\"Decision Boundary for two features used in Decision Tree\")\n",
        "  # plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsxUs3oYbFNK"
      },
      "source": [
        "## Loading IRIS Dataset:\n",
        "\n",
        "### About the IRIS dataset:\n",
        "\n",
        "The Iris Dataset contains four features (length and width of sepals and petals) of 50 samples of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). We shall be using decision trees to try to predict the correct species of the flower using these four features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VR4gNQ5Vuwk"
      },
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "irisData = pandas.DataFrame(\\\n",
        "    data = np.hstack((X,y.reshape(y.shape[0], 1), [[iris[\"target_names\"][int(classIdx)]] for classIdx in y])), \\\n",
        "    columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', \"Class\", \"ClassName\"])\n",
        "irisData.sample(n = 10, random_state = 1)\n",
        "\n",
        "#Here is a few samples: The dataset has 4 non-catagorical features and a class which can take of one of the three values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oN3nv1ExEcJ"
      },
      "source": [
        "## Example of DT on Iris dataset with performace evaluation, and tree structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxwk-zEuqc9I"
      },
      "source": [
        "giveAnExample(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkrm75u1x9RX"
      },
      "source": [
        "### Task 1:\n",
        "Use the above tree to evaluate the classes for the following examples and find the accuracy over these 5 samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k31oDgYlw2cg"
      },
      "source": [
        "irisData.sample(n = 5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afEhTjh9jCT2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjTv9DSA1zbg"
      },
      "source": [
        "Now let us see how we perform when we try to have a more complex decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKKqh4_BzB4K"
      },
      "source": [
        "giveAnExample(3)\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Combine the selected samples into a DataFrame\n",
        "df_samples = pd.DataFrame(X_sample, columns=iris.feature_names)\n",
        "df_samples['True Class'] = [iris.target_names[label] for label in y_sample]\n",
        "df_samples['Predicted Class'] = [iris.target_names[label] for label in y_pred_sample]\n",
        "\n",
        "# Display the interactive table\n",
        "display(df_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2VZZb0EzZUSh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS7wSLX5zTIr"
      },
      "source": [
        "### Task 2:\n",
        "Repeat Exercise 1 for the above tree as well.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We observe that even though that the tree had four features available to it, the tree uses only two of them to classify the cases of species. It gives us an idea that those two features chosen are performing quite decently. Let us examine the decision boundary generated by the tree when only those two features namely **petal length and petal width** are used"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Function to plot the decision boundary\n",
        "def plotDecisionBoundary(X, y, clf):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "    y_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    y_pred = y_pred.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.contourf(xx, yy, y_pred, alpha=0.4)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')\n",
        "    plt.title(\"Decision Boundary\")\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.show()\n",
        "\n",
        "# Task 1: Using petal length and petal width (features 2 and 3)\n",
        "pair1 = [2, 3]  # Petal length and petal width\n",
        "X_petal = X[:, pair1]  # Selecting only these two features\n",
        "\n",
        "# Train the decision tree classifier with max_depth=3\n",
        "clf1 = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\n",
        "clf1.fit(X_petal, y)\n",
        "\n",
        "# Plot decision boundary for petal length and petal width\n",
        "plotDecisionBoundary(X_petal, y, clf1)\n",
        "\n",
        "# Task 2: Using sepal length and sepal width (features 0 and 1)\n",
        "pair2 = [0, 1]  # Sepal length and sepal width\n",
        "X_sepal = X[:, pair2]  # Selecting only these two features\n",
        "\n",
        "# Train the decision tree classifier with max_depth=3\n",
        "clf2 = tree.DecisionTreeClassifier(random_state=0, max_depth=3)\n",
        "clf2.fit(X_sepal, y)\n",
        "\n",
        "# Plot decision boundary for sepal length and sepal width\n",
        "plotDecisionBoundary(X_sepal, y, clf2)"
      ],
      "metadata": {
        "id": "e9WnZHASZyk5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJN66PO-4Q0h"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [2, 3]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, [2, 3], clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQ6VO7J78vp"
      },
      "source": [
        "**Decision boundary** with considering **sepal width and length**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKnuGzgf6eoZ"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [0, 1]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fVoKPWL8W-B"
      },
      "source": [
        "**Decision boundary** with considering **sepal length and pedal length**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXSDC6Il7wDA"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [0, 2]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzKiQd5Y8ox5"
      },
      "source": [
        "**Decision boundary** with considering **sepal width and pedal width**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6StxQVn8i01"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
        "pair = [1, 3]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, pair, clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uuefe-k_dAO"
      },
      "source": [
        "---\n",
        "\n",
        "### Task 3:\n",
        "\n",
        "#### 3.1 :\n",
        "We see that the above decision boundaries are with depth of 3. Compare the above boundary with trees that have higher complexity (by changing the value of `max_depth`) and report your observations. also compare the accuracies for different values of max_depth\n",
        "\n",
        "Test with `max_depth` of the following values:\n",
        "- 2\n",
        "- 5\n",
        "- 8\n",
        "- 10\n",
        "\n",
        "#### 3.2 :\n",
        "\n",
        "On a closer look, we see that the decision boundaries' lines are always at a right angle to the principle axes. Can you reason on why is that the case? \\\n",
        "`(Hint: How is a decision made at any node?)`\n",
        "\n",
        "---\n",
        "\n",
        "### Task 4:\n",
        "\n",
        "#### 4.1 :\n",
        "Complete the following function predict: which takes in four variables : `sepal width, sepal length, petal width, petal length` and returns the class of the flower.\n",
        "\n",
        "#### 4.2 :\n",
        "Use the decision tree made in Exercise 2 and report the logic using multiple nested `if else` statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAIeeEFY84oA"
      },
      "source": [
        "def predictSpecies(sepal_width, sepal_length, petal_width,  petal_length) -> str :\n",
        "  \"\"\"\n",
        "    Write your program here to return the species of the plant (string) using if else statements.\n",
        "  \"\"\"\n",
        "  pass\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Function to plot the decision boundary\n",
        "def plotDecisionBoundary(X, y, clf):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "    y_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    y_pred = y_pred.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.contourf(xx, yy, y_pred, alpha=0.4)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')\n",
        "    plt.title(f\"Decision Boundary with max_depth={clf.max_depth}\")\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.show()\n",
        "\n",
        "# Select features (e.g., petal length and petal width)\n",
        "pair = [2, 3]  # Petal length and petal width\n",
        "X_selected = X[:, pair]\n",
        "\n",
        "# Function to train, evaluate, and plot for different max_depth values\n",
        "def evaluate_max_depth(depth_values):\n",
        "    for depth in depth_values:\n",
        "        clf = tree.DecisionTreeClassifier(random_state=0, max_depth=depth)\n",
        "        clf.fit(X_selected, y)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        y_pred = clf.predict(X_selected)\n",
        "        accuracy = accuracy_score(y, y_pred)\n",
        "\n",
        "        print(f\"Accuracy for max_depth={depth}: {accuracy:.2f}\")\n",
        "\n",
        "        # Plot decision boundary\n",
        "        plotDecisionBoundary(X_selected, y, clf)\n",
        "\n",
        "# Test with max_depth values of 2, 5, 8, and 10\n",
        "depth_values = [2, 5, 8, 10]\n",
        "evaluate_max_depth(depth_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4agDMEMCNeB-"
      },
      "source": [
        "# Entropy and Information:\n",
        "\n",
        "## How are decision trees built?\n",
        "\n",
        "A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous).\n",
        "We use entropy to calculate the homogeneity of a sample.\n",
        "\n",
        "Entropy itself is defined in the following way:\n",
        "\n",
        "$$E(s) = \\sum_{i=1}^c - p_i * log_2(p_i)$$\n",
        "\n",
        "Where $i$ iterates through the classes of the current group and $p_i$ is the probability of choosing an item from class $i$ when a datapoint is randomly picked from the group.\n",
        "\n",
        "At anypoint in the process of making the decision tree. All possible methods of dividing the group are considered (across all features and values of separations) and then the division with the most amount of **Information Gain** is used to divide the current group into two. This is done recursively to finally attain a tree.\n",
        "\n",
        "Here Information Gain is defined by the difference in Entropy of the group before the division and the weighted sum of the entropy of the two groups after division.\n",
        "\n",
        "$$IG(X) = E(s) - E(s, X)$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgnd4qhrM1Km"
      },
      "source": [
        "irisData.sample(n = 10, random_state = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMDVOKZRY1GE"
      },
      "source": [
        "## Task 5:\n",
        "Calculate the Entropy of the above collection of 10 datapoints.\n",
        "## Task 6:\n",
        "#### 6.1 :\n",
        "Suggest a decision node (if, else) statement which divides the group into two groups.\n",
        "#### 6.2 :\n",
        "Also compute the Information Gain in that division step.\n",
        "#### 6.3 :\n",
        "Compare this with other decision clauses that you can make and intuitively comment on which is better for classification and observe if this has any correlation with the numerical value of Information Gain.\n",
        "\n",
        "---\n",
        "\n",
        "End of Lab 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming irisData is already created as shown in your previous code\n",
        "sample_data = irisData.sample(n=10, random_state=5)\n",
        "\n",
        "# Calculate the class distribution (proportions of each class)\n",
        "class_counts = sample_data['ClassName'].value_counts()\n",
        "class_proportions = class_counts / len(sample_data)\n",
        "\n",
        "# Calculate the entropy using the formula\n",
        "entropy = -np.sum(class_proportions * np.log2(class_proportions))\n",
        "\n",
        "print(f\"Entropy of the sample: {entropy:.4f}\")\n",
        "Entropy of the sample: 1.5710\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming irisData is already created as shown in your previous code\n",
        "sample_data = irisData.sample(n=10, random_state=5)\n",
        "\n",
        "# Calculate the class distribution (proportions of each class)\n",
        "class_counts = sample_data['ClassName'].value_counts()\n",
        "class_proportions = class_counts / len(sample_data)\n",
        "\n",
        "# Calculate the entropy using the formula\n",
        "entropy = -np.sum(class_proportions * np.log2(class_proportions))\n",
        "\n",
        "print(f\"Entropy of the sample: {entropy:.4f}\")\n",
        "\n",
        "     \n",
        "Entropy of the sample: 1.5710"
      ],
      "metadata": {
        "id": "tNFsTpKTaJPW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTrqJ86cYcFL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}